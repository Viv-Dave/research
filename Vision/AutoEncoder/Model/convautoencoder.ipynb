{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-10-24T10:58:21.234464Z","iopub.status.busy":"2025-10-24T10:58:21.234218Z","iopub.status.idle":"2025-10-24T10:58:31.265543Z","shell.execute_reply":"2025-10-24T10:58:31.264779Z","shell.execute_reply.started":"2025-10-24T10:58:21.234445Z"},"id":"4PI8BhXV5qb_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n\n    ]\n)","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:31.267064Z","iopub.status.busy":"2025-10-24T10:58:31.266724Z","iopub.status.idle":"2025-10-24T10:58:31.27091Z","shell.execute_reply":"2025-10-24T10:58:31.270375Z","shell.execute_reply.started":"2025-10-24T10:58:31.267048Z"},"id":"61bd8mQT5qcN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = datasets.MNIST(root='kaggle/input', train=True, transform=transform, download=True)","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:31.271821Z","iopub.status.busy":"2025-10-24T10:58:31.271559Z","iopub.status.idle":"2025-10-24T10:58:33.551542Z","shell.execute_reply":"2025-10-24T10:58:33.550858Z","shell.execute_reply.started":"2025-10-24T10:58:31.2718Z"},"id":"b4Ho0QN05qcO","outputId":"724762e5-8e3a-4af8-ce59-8241f7920dcd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = datasets.MNIST(root='kaggle/input', train=False,transform=transform, download=True)","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.552496Z","iopub.status.busy":"2025-10-24T10:58:33.552293Z","iopub.status.idle":"2025-10-24T10:58:33.566556Z","shell.execute_reply":"2025-10-24T10:58:33.56597Z","shell.execute_reply.started":"2025-10-24T10:58:33.552479Z"},"id":"00n0bQHo5qcP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, shuffle=True, num_workers=2, batch_size=64)\ntest_loader = DataLoader(test_dataset, shuffle=False, num_workers=2, batch_size=64)","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.568811Z","iopub.status.busy":"2025-10-24T10:58:33.568547Z","iopub.status.idle":"2025-10-24T10:58:33.57277Z","shell.execute_reply":"2025-10-24T10:58:33.572031Z","shell.execute_reply.started":"2025-10-24T10:58:33.568795Z"},"id":"LXdfdMwA5qcQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = train_dataset[0]\nimage[0].shape","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.573653Z","iopub.status.busy":"2025-10-24T10:58:33.573397Z","iopub.status.idle":"2025-10-24T10:58:33.594848Z","shell.execute_reply":"2025-10-24T10:58:33.594261Z","shell.execute_reply.started":"2025-10-24T10:58:33.573632Z"},"id":"4uKWtezV5qcQ","outputId":"e08713b8-1166-4af0-9a1a-7796e2ee8ba4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nConstructing the Encoder of AutoEncoder\n\"\"\"\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(4) #28 x 28\n\n        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(8)#14 x 14\n\n        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(16)#7 x 7\n\n        self.pool = nn.MaxPool2d(2,2)\n        self.lm_head = nn.Sequential(\n            nn.Linear(16*7*7, 512),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 256),\n            nn.GELU(),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Linear(128, 64),\n            nn.GELU(),\n            nn.Linear(64, 9),\n        )\n\n        self.fc1 = nn.Linear(in_features=16 * 7 * 7, out_features=9)\n\n    def forward(self,x):\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = F.relu(self.bn3(self.conv3(x)))\n\n        x = torch.flatten(x, 1)\n        x = self.lm_head(x)\n        return x","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.595843Z","iopub.status.busy":"2025-10-24T10:58:33.595573Z","iopub.status.idle":"2025-10-24T10:58:33.61002Z","shell.execute_reply":"2025-10-24T10:58:33.609339Z","shell.execute_reply.started":"2025-10-24T10:58:33.595822Z"},"id":"gdpmhIQo5qcQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.lm_head = nn.Sequential(\n            nn.Linear(9, 64),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 128),\n            nn.GELU(),\n            nn.Linear(128, 256),\n            nn.GELU(),\n            nn.Linear(256, 512),\n            nn.GELU(),\n            nn.Linear(512, 16 * 7 * 7)\n        )\n\n        self.deconv1 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=2, stride=2)\n        self.bn1 = nn.BatchNorm2d(8)\n\n        # Block 2: 14x14 -> 28x28\n        self.deconv2 = nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=2, stride=2)\n        self.bn2 = nn.BatchNorm2d(4)\n\n        # Final Layer: to get back to 1 channel\n        self.final_conv = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=3, padding=1)\n    def forward(self,x):\n        x = self.lm_head(x)\n        x = x.view(-1, 16, 7, 7)\n\n        x = F.relu(self.bn1(self.deconv1(x)))\n\n\n        x = F.relu(self.bn2(self.deconv2(x)))\n\n        x = torch.sigmoid(self.final_conv(x))\n\n        return x","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.61107Z","iopub.status.busy":"2025-10-24T10:58:33.610748Z","iopub.status.idle":"2025-10-24T10:58:33.628719Z","shell.execute_reply":"2025-10-24T10:58:33.628192Z","shell.execute_reply.started":"2025-10-24T10:58:33.611048Z"},"id":"kjXK8i8C5qcR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n    def forward(self,x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n\n        return x","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.629717Z","iopub.status.busy":"2025-10-24T10:58:33.629335Z","iopub.status.idle":"2025-10-24T10:58:33.649353Z","shell.execute_reply":"2025-10-24T10:58:33.648844Z","shell.execute_reply.started":"2025-10-24T10:58:33.629702Z"},"id":"gqjbRLUq5qcS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoEncoder().to(device)","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.650193Z","iopub.status.busy":"2025-10-24T10:58:33.649977Z","iopub.status.idle":"2025-10-24T10:58:33.978234Z","shell.execute_reply":"2025-10-24T10:58:33.977452Z","shell.execute_reply.started":"2025-10-24T10:58:33.650175Z"},"id":"kPZPym5A5qcS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_EPOCHS = 15\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.979177Z","iopub.status.busy":"2025-10-24T10:58:33.978999Z","iopub.status.idle":"2025-10-24T10:58:33.982742Z","shell.execute_reply":"2025-10-24T10:58:33.982195Z","shell.execute_reply.started":"2025-10-24T10:58:33.979163Z"},"id":"8xC249CC5qcT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.L1Loss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.execute_input":"2025-10-24T10:58:33.983638Z","iopub.status.busy":"2025-10-24T10:58:33.983402Z","iopub.status.idle":"2025-10-24T10:58:34.000535Z","shell.execute_reply":"2025-10-24T10:58:33.99978Z","shell.execute_reply.started":"2025-10-24T10:58:33.983618Z"},"id":"O3ELCT9z5qcT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(NUM_EPOCHS):\n    model.train()\n    running_train_loss = 0.0\n    for data in train_loader:\n\n        images, _ = data\n        images = images.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, images)\n        loss.backward()\n        optimizer.step()\n\n        running_train_loss += loss.item() * images.size(0)\n\n    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n\n    model.eval()\n    running_test_loss = 0.0\n    with torch.no_grad():\n        for data in test_loader:\n            images, _ = data\n            images = images.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, images)\n\n            running_test_loss += loss.item() * images.size(0)\n\n    epoch_test_loss = running_test_loss / len(test_loader.dataset)\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {epoch_train_loss:.6f}, Test Loss: {epoch_test_loss:.6f}\")\n\nprint(\"\\nTraining finished.\")","metadata":{"execution":{"iopub.execute_input":"2025-10-24T11:00:35.405391Z","iopub.status.busy":"2025-10-24T11:00:35.404688Z","iopub.status.idle":"2025-10-24T11:01:52.069959Z","shell.execute_reply":"2025-10-24T11:01:52.06876Z","shell.execute_reply.started":"2025-10-24T11:00:35.405369Z"},"id":"P0sRhUHJ5qcT","outputId":"f36b4a46-53db-431b-80d6-1fd98f86cd8c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n# --- 5. VISUALIZE RESULTS ON RANDOM SAMPLES ---\n\n# Put the model in evaluation mode\nmodel.eval()\n\n# --- Select Random Images from the Test Dataset ---\nnum_images_to_show = 10\nnum_samples_in_test = len(test_dataset)\n\n# Generate a list of random indices\nrandom_indices = random.sample(range(num_samples_in_test), num_images_to_show)\n\n# Create a custom batch containing only the randomly selected images\n# We get the image tensor (index 0) from the dataset for each random index\nrandom_test_images = torch.stack([test_dataset[i][0] for i in random_indices])\nrandom_test_images = random_test_images.to(device)\n\n# Get the reconstructed images for our random batch\nwith torch.no_grad():\n    reconstructed_images = model(random_test_images)\n\n# Move tensors to the CPU for plotting\nimages_np = random_test_images.cpu().numpy()\noutputs_np = reconstructed_images.cpu().numpy()\n\n# --- Plotting ---\nfig, axes = plt.subplots(nrows=num_images_to_show, ncols=2, figsize=(5, 20))\nfig.suptitle('Original vs. Reconstructed (Random Samples)', fontsize=16)\n\nfor i in range(num_images_to_show):\n    # --- Display Original Image ---\n    ax_orig = axes[i, 0]\n    ax_orig.imshow(np.squeeze(images_np[i]), cmap='gray')\n    ax_orig.set_title(\"Original\")\n    ax_orig.get_xaxis().set_visible(False)\n    ax_orig.get_yaxis().set_visible(False)\n\n    # --- Display Reconstructed Image ---\n    ax_recon = axes[i, 1]\n    ax_recon.imshow(np.squeeze(outputs_np[i]), cmap='gray')\n    ax_recon.set_title(\"Reconstructed\")\n    ax_recon.get_xaxis().set_visible(False)\n    ax_recon.get_yaxis().set_visible(False)\n\n# Adjust layout and show the plot\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-10-24T11:01:57.911331Z","iopub.status.busy":"2025-10-24T11:01:57.910521Z","iopub.status.idle":"2025-10-24T11:01:58.463989Z","shell.execute_reply":"2025-10-24T11:01:58.463253Z","shell.execute_reply.started":"2025-10-24T11:01:57.911301Z"},"id":"PQVsst_E5qcU","outputId":"190afce9-9b2e-46e4-e91b-4fd0b03b67bb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Put the model in evaluation mode\nmodel.eval()\n\n# Initialize an empty list to store the encoded outputs\nencoded_representations = []\ntest_labels = []\n\n# Iterate through the test_loader\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        images = images.to(device)\n\n        # Pass the images through the model.encoder\n        encoded_output = model.encoder(images)\n\n        # Extend the encoded_representations list\n        encoded_representations.extend(encoded_output.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n\n# Convert the encoded_representations list into a NumPy array\nencoded_representations_np = np.array(encoded_representations)\ntest_labels_np = np.array(test_labels)\n\nprint(\"Shape of encoded representations:\", encoded_representations_np.shape)\nprint(\"Shape of test labels:\", test_labels_np.shape)","metadata":{"id":"e8e5703a","outputId":"3d95a327-5f60-436a-9c96-a254bd26bf41"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply t-sne\n\n### Subtask:\nUse scikit-learn's `TSNE` to reduce the dimensionality of the encoded representations to 2.\n","metadata":{"id":"49e2a4de"}},{"cell_type":"markdown","source":"**Reasoning**:\nUse scikit-learn's TSNE to reduce the dimensionality of the encoded representations to 2.\n\n","metadata":{"id":"22951860"}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\ntsne_results = tsne.fit_transform(encoded_representations_np)\n\nprint(\"Shape of t-SNE results:\", tsne_results.shape)","metadata":{"id":"b3292447","outputId":"4b9a7768-73a3-4c74-9cfa-145d7c027fee"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize t-sne results\n\n### Subtask:\nPlot the 2-dimensional t-SNE output, coloring the points by their original digit labels.\n","metadata":{"id":"a40fb244"}},{"cell_type":"markdown","source":"**Reasoning**:\nPlot the 2-dimensional t-SNE output, coloring the points by their original digit labels, and add a title and legend.\n\n","metadata":{"id":"926b7aae"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=test_labels_np, cmap='tab10', s=10)\nplt.title(\"t-SNE Visualization of Encoded MNIST Data\")\nplt.colorbar(scatter, label=\"Digit Label\")\nplt.show()","metadata":{"id":"85d6dd28","outputId":"400652d4-e2dd-4841-aee4-03e598a50daf"},"outputs":[],"execution_count":null}]}