{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e9981c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-17T11:41:48.630169Z",
     "iopub.status.busy": "2025-08-17T11:41:48.629957Z",
     "iopub.status.idle": "2025-08-17T11:41:54.271684Z",
     "shell.execute_reply": "2025-08-17T11:41:54.271023Z"
    },
    "papermill": {
     "duration": 5.646251,
     "end_time": "2025-08-17T11:41:54.273106",
     "exception": false,
     "start_time": "2025-08-17T11:41:48.626855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634d8bcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:41:54.278457Z",
     "iopub.status.busy": "2025-08-17T11:41:54.278145Z",
     "iopub.status.idle": "2025-08-17T11:41:54.630663Z",
     "shell.execute_reply": "2025-08-17T11:41:54.629947Z"
    },
    "papermill": {
     "duration": 0.356061,
     "end_time": "2025-08-17T11:41:54.631733",
     "exception": false,
     "start_time": "2025-08-17T11:41:54.275672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/celeba-face-recognition-triplets\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"quadeer15sh/celeba-face-recognition-triplets\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd33724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:41:54.636737Z",
     "iopub.status.busy": "2025-08-17T11:41:54.636526Z",
     "iopub.status.idle": "2025-08-17T11:41:59.544146Z",
     "shell.execute_reply": "2025-08-17T11:41:59.543539Z"
    },
    "papermill": {
     "duration": 4.911778,
     "end_time": "2025-08-17T11:41:59.545672",
     "exception": false,
     "start_time": "2025-08-17T11:41:54.633894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from torchvision import transforms \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir,'images')\n",
    "        self.annotations = pd.read_csv(root_dir+'triplets.csv')\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.annotations.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        anchor_img = self.annotations.iloc[index,0]\n",
    "        positive_img = self.annotations.iloc[index,2]\n",
    "        negative_img = self.annotations.iloc[index,4]\n",
    "\n",
    "        anc_img_path = os.path.join(self.image_dir, anchor_img)\n",
    "        pos_img_path = os.path.join(self.image_dir, positive_img)\n",
    "        neg_img_path = os.path.join(self.image_dir, negative_img)\n",
    "\n",
    "        anc_image = Image.open(anc_img_path).convert(\"RGB\")\n",
    "        pos_image = Image.open(pos_img_path).convert(\"RGB\")\n",
    "        neg_image = Image.open(neg_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            anc_image = self.transform(anc_image)\n",
    "            pos_image = self.transform(pos_image)\n",
    "            neg_image = self.transform(neg_image)\n",
    "        return anc_image, pos_image, neg_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff69f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:41:59.551409Z",
     "iopub.status.busy": "2025-08-17T11:41:59.551020Z",
     "iopub.status.idle": "2025-08-17T11:42:01.022744Z",
     "shell.execute_reply": "2025-08-17T11:42:01.021904Z"
    },
    "papermill": {
     "duration": 1.475833,
     "end_time": "2025-08-17T11:42:01.023957",
     "exception": false,
     "start_time": "2025-08-17T11:41:59.548124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16332 triplets.\n",
      "torch.Size([3, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "path = '/kaggle/input/celeba-face-recognition-triplets/CelebA FR Triplets/CelebA FR Triplets/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "try:\n",
    "    dataset = TripletDataset(root_dir=path, transform=transform)\n",
    "    print(f\"Found {len(dataset)} triplets.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load dataset. Error: {e}\")\n",
    "\n",
    "\n",
    "train_indice, test_indice = train_test_split(list(range(len(dataset))),test_size=0.2, random_state=42)\n",
    "train_subset, test_subset = Subset(dataset, train_indice), Subset(dataset, test_indice)\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True,  num_workers=4,pin_memory=True, prefetch_factor=2)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=True)\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9244e0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:42:01.029429Z",
     "iopub.status.busy": "2025-08-17T11:42:01.029026Z",
     "iopub.status.idle": "2025-08-17T11:42:01.122184Z",
     "shell.execute_reply": "2025-08-17T11:42:01.121486Z"
    },
    "papermill": {
     "duration": 0.097187,
     "end_time": "2025-08-17T11:42:01.123414",
     "exception": false,
     "start_time": "2025-08-17T11:42:01.026227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class FaceRecognition(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceRecognition,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=8, kernel_size=3, padding=1) #112*112\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=16, kernel_size=3, padding=1) #56*56\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels=32, kernel_size=3, padding=1) #28*28\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) #14*14\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1) #7*7\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=128 * 3 * 3, out_features=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "\n",
    "        return x\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6165af63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:42:01.128635Z",
     "iopub.status.busy": "2025-08-17T11:42:01.128191Z",
     "iopub.status.idle": "2025-08-17T11:42:01.339780Z",
     "shell.execute_reply": "2025-08-17T11:42:01.339253Z"
    },
    "papermill": {
     "duration": 0.215489,
     "end_time": "2025-08-17T11:42:01.341109",
     "exception": false,
     "start_time": "2025-08-17T11:42:01.125620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from numpy.linalg import norm as norm\n",
    "import torch.nn as nn\n",
    "model = FaceRecognition().to(device)\n",
    "# def triplet_loss():\n",
    "#     alpha = 0.2 #A hyperparameter\n",
    "#     loss = norm(anc_vec - pos_vec)-norm(anc_vec-neg_vec)+alpha\n",
    "#     return loss\n",
    "NUM_EPOCHS = 25\n",
    "criterion = nn.TripletMarginLoss(margin=0.2, p=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e58190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:42:01.347530Z",
     "iopub.status.busy": "2025-08-17T11:42:01.347323Z",
     "iopub.status.idle": "2025-08-17T11:51:58.252814Z",
     "shell.execute_reply": "2025-08-17T11:51:58.251689Z"
    },
    "papermill": {
     "duration": 596.91059,
     "end_time": "2025-08-17T11:51:58.254126",
     "exception": false,
     "start_time": "2025-08-17T11:42:01.343536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [100/409], Loss: 0.1333\n",
      "Epoch [1/25], Step [200/409], Loss: 0.1140\n",
      "Epoch [1/25], Step [300/409], Loss: 0.0986\n",
      "Epoch [1/25], Step [400/409], Loss: 0.0963\n",
      "Epoch [2/25], Step [100/409], Loss: 0.0766\n",
      "Epoch [2/25], Step [200/409], Loss: 0.0744\n",
      "Epoch [2/25], Step [300/409], Loss: 0.0768\n",
      "Epoch [2/25], Step [400/409], Loss: 0.0718\n",
      "Epoch [3/25], Step [100/409], Loss: 0.0550\n",
      "Epoch [3/25], Step [200/409], Loss: 0.0576\n",
      "Epoch [3/25], Step [300/409], Loss: 0.0573\n",
      "Epoch [3/25], Step [400/409], Loss: 0.0584\n",
      "Epoch [4/25], Step [100/409], Loss: 0.0423\n",
      "Epoch [4/25], Step [200/409], Loss: 0.0445\n",
      "Epoch [4/25], Step [300/409], Loss: 0.0447\n",
      "Epoch [4/25], Step [400/409], Loss: 0.0458\n",
      "Epoch [5/25], Step [100/409], Loss: 0.0332\n",
      "Epoch [5/25], Step [200/409], Loss: 0.0306\n",
      "Epoch [5/25], Step [300/409], Loss: 0.0357\n",
      "Epoch [5/25], Step [400/409], Loss: 0.0336\n",
      "Epoch [6/25], Step [100/409], Loss: 0.0222\n",
      "Epoch [6/25], Step [200/409], Loss: 0.0250\n",
      "Epoch [6/25], Step [300/409], Loss: 0.0277\n",
      "Epoch [6/25], Step [400/409], Loss: 0.0248\n",
      "Epoch [7/25], Step [100/409], Loss: 0.0159\n",
      "Epoch [7/25], Step [200/409], Loss: 0.0170\n",
      "Epoch [7/25], Step [300/409], Loss: 0.0195\n",
      "Epoch [7/25], Step [400/409], Loss: 0.0186\n",
      "Epoch [8/25], Step [100/409], Loss: 0.0120\n",
      "Epoch [8/25], Step [200/409], Loss: 0.0134\n",
      "Epoch [8/25], Step [300/409], Loss: 0.0133\n",
      "Epoch [8/25], Step [400/409], Loss: 0.0126\n",
      "Epoch [9/25], Step [100/409], Loss: 0.0092\n",
      "Epoch [9/25], Step [200/409], Loss: 0.0073\n",
      "Epoch [9/25], Step [300/409], Loss: 0.0086\n",
      "Epoch [9/25], Step [400/409], Loss: 0.0092\n",
      "Epoch [10/25], Step [100/409], Loss: 0.0057\n",
      "Epoch [10/25], Step [200/409], Loss: 0.0054\n",
      "Epoch [10/25], Step [300/409], Loss: 0.0057\n",
      "Epoch [10/25], Step [400/409], Loss: 0.0065\n",
      "Epoch [11/25], Step [100/409], Loss: 0.0035\n",
      "Epoch [11/25], Step [200/409], Loss: 0.0038\n",
      "Epoch [11/25], Step [300/409], Loss: 0.0034\n",
      "Epoch [11/25], Step [400/409], Loss: 0.0043\n",
      "Epoch [12/25], Step [100/409], Loss: 0.0026\n",
      "Epoch [12/25], Step [200/409], Loss: 0.0027\n",
      "Epoch [12/25], Step [300/409], Loss: 0.0027\n",
      "Epoch [12/25], Step [400/409], Loss: 0.0028\n",
      "Epoch [13/25], Step [100/409], Loss: 0.0022\n",
      "Epoch [13/25], Step [200/409], Loss: 0.0019\n",
      "Epoch [13/25], Step [300/409], Loss: 0.0021\n",
      "Epoch [13/25], Step [400/409], Loss: 0.0017\n",
      "Epoch [14/25], Step [100/409], Loss: 0.0015\n",
      "Epoch [14/25], Step [200/409], Loss: 0.0013\n",
      "Epoch [14/25], Step [300/409], Loss: 0.0016\n",
      "Epoch [14/25], Step [400/409], Loss: 0.0011\n",
      "Epoch [15/25], Step [100/409], Loss: 0.0011\n",
      "Epoch [15/25], Step [200/409], Loss: 0.0012\n",
      "Epoch [15/25], Step [300/409], Loss: 0.0009\n",
      "Epoch [15/25], Step [400/409], Loss: 0.0008\n",
      "Epoch [16/25], Step [100/409], Loss: 0.0006\n",
      "Epoch [16/25], Step [200/409], Loss: 0.0006\n",
      "Epoch [16/25], Step [300/409], Loss: 0.0007\n",
      "Epoch [16/25], Step [400/409], Loss: 0.0008\n",
      "Epoch [17/25], Step [100/409], Loss: 0.0005\n",
      "Epoch [17/25], Step [200/409], Loss: 0.0005\n",
      "Epoch [17/25], Step [300/409], Loss: 0.0005\n",
      "Epoch [17/25], Step [400/409], Loss: 0.0006\n",
      "Epoch [18/25], Step [100/409], Loss: 0.0004\n",
      "Epoch [18/25], Step [200/409], Loss: 0.0005\n",
      "Epoch [18/25], Step [300/409], Loss: 0.0004\n",
      "Epoch [18/25], Step [400/409], Loss: 0.0003\n",
      "Epoch [19/25], Step [100/409], Loss: 0.0002\n",
      "Epoch [19/25], Step [200/409], Loss: 0.0003\n",
      "Epoch [19/25], Step [300/409], Loss: 0.0002\n",
      "Epoch [19/25], Step [400/409], Loss: 0.0003\n",
      "Epoch [20/25], Step [100/409], Loss: 0.0002\n",
      "Epoch [20/25], Step [200/409], Loss: 0.0002\n",
      "Epoch [20/25], Step [300/409], Loss: 0.0003\n",
      "Epoch [20/25], Step [400/409], Loss: 0.0001\n",
      "Epoch [21/25], Step [100/409], Loss: 0.0002\n",
      "Epoch [21/25], Step [200/409], Loss: 0.0002\n",
      "Epoch [21/25], Step [300/409], Loss: 0.0002\n",
      "Epoch [21/25], Step [400/409], Loss: 0.0002\n",
      "Epoch [22/25], Step [100/409], Loss: 0.0002\n",
      "Epoch [22/25], Step [200/409], Loss: 0.0001\n",
      "Epoch [22/25], Step [300/409], Loss: 0.0001\n",
      "Epoch [22/25], Step [400/409], Loss: 0.0001\n",
      "Epoch [23/25], Step [100/409], Loss: 0.0001\n",
      "Epoch [23/25], Step [200/409], Loss: 0.0001\n",
      "Epoch [23/25], Step [300/409], Loss: 0.0001\n",
      "Epoch [23/25], Step [400/409], Loss: 0.0001\n",
      "Epoch [24/25], Step [100/409], Loss: 0.0001\n",
      "Epoch [24/25], Step [200/409], Loss: 0.0001\n",
      "Epoch [24/25], Step [300/409], Loss: 0.0001\n",
      "Epoch [24/25], Step [400/409], Loss: 0.0001\n",
      "Epoch [25/25], Step [100/409], Loss: 0.0001\n",
      "Epoch [25/25], Step [200/409], Loss: 0.0001\n",
      "Epoch [25/25], Step [300/409], Loss: 0.0001\n",
      "Epoch [25/25], Step [400/409], Loss: 0.0001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 25\n",
    "model.train()\n",
    "for epochs in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        anc_images, pos_images, neg_images = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        anc_embedding = model(anc_images)\n",
    "        pos_embedding = model(pos_images)\n",
    "        neg_embedding = model(neg_images)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(anc_embedding, pos_embedding, neg_embedding)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0: \n",
    "            print(f'Epoch [{epochs + 1}/{NUM_EPOCHS}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ccb34a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:51:58.267309Z",
     "iopub.status.busy": "2025-08-17T11:51:58.266551Z",
     "iopub.status.idle": "2025-08-17T11:51:58.281479Z",
     "shell.execute_reply": "2025-08-17T11:51:58.280759Z"
    },
    "papermill": {
     "duration": 0.02258,
     "end_time": "2025-08-17T11:51:58.282578",
     "exception": false,
     "start_time": "2025-08-17T11:51:58.259998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/kaggle/working/my_model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1b36e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T11:51:58.294670Z",
     "iopub.status.busy": "2025-08-17T11:51:58.294462Z",
     "iopub.status.idle": "2025-08-17T11:52:44.798540Z",
     "shell.execute_reply": "2025-08-17T11:52:44.797736Z"
    },
    "papermill": {
     "duration": 46.516505,
     "end_time": "2025-08-17T11:52:44.804751",
     "exception": false,
     "start_time": "2025-08-17T11:51:58.288246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Simple Evaluation on Test Set ---\n",
      "Average Test Loss: 0.0711\n",
      "Triplet Accuracy: 86.38%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"\\n--- Running Simple Evaluation on Test Set ---\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "correct_triplets = 0\n",
    "total_triplets = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        anc_images, pos_images, neg_images = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        \n",
    "        anc_embedding = model(anc_images)\n",
    "        pos_embedding = model(pos_images)\n",
    "        neg_embedding = model(neg_images)\n",
    "        \n",
    "        loss = criterion(anc_embedding, pos_embedding, neg_embedding)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        dist_pos = torch.norm(anc_embedding - pos_embedding, dim=1)\n",
    "        dist_neg = torch.norm(anc_embedding - neg_embedding, dim=1)\n",
    "    \n",
    "        correct_triplets += torch.sum(dist_pos < dist_neg).item()\n",
    "        total_triplets += anc_images.size(0)\n",
    "\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = 100 * correct_triplets / total_triplets\n",
    "\n",
    "print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Triplet Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3713827,
     "sourceId": 6435504,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2937277,
     "sourceId": 6531447,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 663.739975,
   "end_time": "2025-08-17T11:52:48.234353",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-17T11:41:44.494378",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
