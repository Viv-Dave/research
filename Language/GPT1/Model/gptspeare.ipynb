{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1932364c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-30T17:34:57.747008Z",
     "iopub.status.busy": "2025-09-30T17:34:57.746767Z",
     "iopub.status.idle": "2025-09-30T17:35:04.408369Z",
     "shell.execute_reply": "2025-09-30T17:35:04.407738Z"
    },
    "papermill": {
     "duration": 6.667541,
     "end_time": "2025-09-30T17:35:04.409696",
     "exception": false,
     "start_time": "2025-09-30T17:34:57.742155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fae4abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:04.417205Z",
     "iopub.status.busy": "2025-09-30T17:35:04.416533Z",
     "iopub.status.idle": "2025-09-30T17:35:04.502171Z",
     "shell.execute_reply": "2025-09-30T17:35:04.501586Z"
    },
    "papermill": {
     "duration": 0.090278,
     "end_time": "2025-09-30T17:35:04.503336",
     "exception": false,
     "start_time": "2025-09-30T17:35:04.413058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c048ad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:04.510052Z",
     "iopub.status.busy": "2025-09-30T17:35:04.509847Z",
     "iopub.status.idle": "2025-09-30T17:35:04.519376Z",
     "shell.execute_reply": "2025-09-30T17:35:04.518693Z"
    },
    "papermill": {
     "duration": 0.013952,
     "end_time": "2025-09-30T17:35:04.520400",
     "exception": false,
     "start_time": "2025-09-30T17:35:04.506448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79ff99b82010>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca255918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:04.526995Z",
     "iopub.status.busy": "2025-09-30T17:35:04.526772Z",
     "iopub.status.idle": "2025-09-30T17:35:04.831612Z",
     "shell.execute_reply": "2025-09-30T17:35:04.830655Z"
    },
    "papermill": {
     "duration": 0.309691,
     "end_time": "2025-09-30T17:35:04.833057",
     "exception": false,
     "start_time": "2025-09-30T17:35:04.523366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH  =\"/kaggle/input/shakespeare6/shakespeare.txt\"\n",
    "\n",
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac40bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:04.841516Z",
     "iopub.status.busy": "2025-09-30T17:35:04.841010Z",
     "iopub.status.idle": "2025-09-30T17:35:04.845029Z",
     "shell.execute_reply": "2025-09-30T17:35:04.844199Z"
    },
    "papermill": {
     "duration": 0.00976,
     "end_time": "2025-09-30T17:35:04.846094",
     "exception": false,
     "start_time": "2025-09-30T17:35:04.836334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset character length:  13593923\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset character length: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0248bfa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:04.852582Z",
     "iopub.status.busy": "2025-09-30T17:35:04.852384Z",
     "iopub.status.idle": "2025-09-30T17:35:04.964288Z",
     "shell.execute_reply": "2025-09-30T17:35:04.963568Z"
    },
    "papermill": {
     "duration": 0.116406,
     "end_time": "2025-09-30T17:35:04.965392",
     "exception": false,
     "start_time": "2025-09-30T17:35:04.848986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016870ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:04.972028Z",
     "iopub.status.busy": "2025-09-30T17:35:04.971669Z",
     "iopub.status.idle": "2025-09-30T17:35:06.556026Z",
     "shell.execute_reply": "2025-09-30T17:35:06.555220Z"
    },
    "papermill": {
     "duration": 1.589062,
     "end_time": "2025-09-30T17:35:06.557395",
     "exception": false,
     "start_time": "2025-09-30T17:35:04.968333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a885ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.564690Z",
     "iopub.status.busy": "2025-09-30T17:35:06.564280Z",
     "iopub.status.idle": "2025-09-30T17:35:06.568704Z",
     "shell.execute_reply": "2025-09-30T17:35:06.568029Z"
    },
    "papermill": {
     "duration": 0.009053,
     "end_time": "2025-09-30T17:35:06.569876",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.560823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c646874f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.576764Z",
     "iopub.status.busy": "2025-09-30T17:35:06.576086Z",
     "iopub.status.idle": "2025-09-30T17:35:06.580440Z",
     "shell.execute_reply": "2025-09-30T17:35:06.579808Z"
    },
    "papermill": {
     "duration": 0.008758,
     "end_time": "2025-09-30T17:35:06.581524",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.572766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad5a489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.588086Z",
     "iopub.status.busy": "2025-09-30T17:35:06.587583Z",
     "iopub.status.idle": "2025-09-30T17:35:06.592967Z",
     "shell.execute_reply": "2025-09-30T17:35:06.592312Z"
    },
    "papermill": {
     "duration": 0.009758,
     "end_time": "2025-09-30T17:35:06.594092",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.584334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.6\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d473023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.601376Z",
     "iopub.status.busy": "2025-09-30T17:35:06.600965Z",
     "iopub.status.idle": "2025-09-30T17:35:06.605242Z",
     "shell.execute_reply": "2025-09-30T17:35:06.604720Z"
    },
    "papermill": {
     "duration": 0.0086,
     "end_time": "2025-09-30T17:35:06.606286",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.597686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c67cad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.612615Z",
     "iopub.status.busy": "2025-09-30T17:35:06.612427Z",
     "iopub.status.idle": "2025-09-30T17:35:06.616212Z",
     "shell.execute_reply": "2025-09-30T17:35:06.615715Z"
    },
    "papermill": {
     "duration": 0.008074,
     "end_time": "2025-09-30T17:35:06.617207",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.609133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(4* n_embd, n_embd),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "415f261b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.623642Z",
     "iopub.status.busy": "2025-09-30T17:35:06.623452Z",
     "iopub.status.idle": "2025-09-30T17:35:06.627321Z",
     "shell.execute_reply": "2025-09-30T17:35:06.626830Z"
    },
    "papermill": {
     "duration": 0.008159,
     "end_time": "2025-09-30T17:35:06.628278",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.620119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self,x):\n",
    "        x = x+self.sa(self.ln1(x))\n",
    "        x = x+self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbff2f2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.634477Z",
     "iopub.status.busy": "2025-09-30T17:35:06.634295Z",
     "iopub.status.idle": "2025-09-30T17:35:06.641207Z",
     "shell.execute_reply": "2025-09-30T17:35:06.640683Z"
    },
    "papermill": {
     "duration": 0.011091,
     "end_time": "2025-09-30T17:35:06.642133",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.631042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            nn.LayerNorm(n_embd),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T=idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # logits = self.linear(token_emb) # (B,T,vocab_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = token_emb+pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size of the tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c0a4c37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.648558Z",
     "iopub.status.busy": "2025-09-30T17:35:06.648067Z",
     "iopub.status.idle": "2025-09-30T17:35:06.941913Z",
     "shell.execute_reply": "2025-09-30T17:35:06.941007Z"
    },
    "papermill": {
     "duration": 0.298389,
     "end_time": "2025-09-30T17:35:06.943256",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.644867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5605d705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:06.955231Z",
     "iopub.status.busy": "2025-09-30T17:35:06.954929Z",
     "iopub.status.idle": "2025-09-30T17:35:11.627034Z",
     "shell.execute_reply": "2025-09-30T17:35:11.626440Z"
    },
    "papermill": {
     "duration": 4.679247,
     "end_time": "2025-09-30T17:35:11.628270",
     "exception": false,
     "start_time": "2025-09-30T17:35:06.949023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e44f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T17:35:11.635367Z",
     "iopub.status.busy": "2025-09-30T17:35:11.634802Z",
     "iopub.status.idle": "2025-09-30T18:08:21.318926Z",
     "shell.execute_reply": "2025-09-30T18:08:21.318079Z"
    },
    "papermill": {
     "duration": 1989.689007,
     "end_time": "2025-09-30T18:08:21.320375",
     "exception": false,
     "start_time": "2025-09-30T17:35:11.631368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4981, val loss 4.4981\n",
      "step 200: train loss 2.4444, val loss 2.4589\n",
      "step 400: train loss 2.3489, val loss 2.3627\n",
      "step 600: train loss 2.0764, val loss 2.0981\n",
      "step 800: train loss 1.8485, val loss 1.8754\n",
      "step 1000: train loss 1.6993, val loss 1.7279\n",
      "step 1200: train loss 1.5810, val loss 1.6145\n",
      "step 1400: train loss 1.4948, val loss 1.5328\n",
      "step 1600: train loss 1.4258, val loss 1.4620\n",
      "step 1800: train loss 1.3753, val loss 1.4155\n",
      "step 2000: train loss 1.3361, val loss 1.3793\n",
      "step 2200: train loss 1.3055, val loss 1.3498\n",
      "step 2400: train loss 1.2887, val loss 1.3311\n",
      "step 2600: train loss 1.2656, val loss 1.3066\n",
      "step 2800: train loss 1.2422, val loss 1.2861\n",
      "step 3000: train loss 1.2296, val loss 1.2737\n",
      "step 3200: train loss 1.2135, val loss 1.2600\n",
      "step 3400: train loss 1.2084, val loss 1.2526\n",
      "step 3600: train loss 1.1937, val loss 1.2424\n",
      "step 3800: train loss 1.1807, val loss 1.2319\n",
      "step 4000: train loss 1.1769, val loss 1.2203\n",
      "step 4200: train loss 1.1685, val loss 1.2151\n",
      "step 4400: train loss 1.1606, val loss 1.2112\n",
      "step 4600: train loss 1.1565, val loss 1.2015\n",
      "step 4800: train loss 1.1426, val loss 1.1948\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % 200 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    if max_iters >= 10000:\n",
    "        learning_rate = 1e-3\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ae7b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:08:21.329772Z",
     "iopub.status.busy": "2025-09-30T18:08:21.329558Z",
     "iopub.status.idle": "2025-09-30T18:08:25.318819Z",
     "shell.execute_reply": "2025-09-30T18:08:25.317730Z"
    },
    "papermill": {
     "duration": 3.995258,
     "end_time": "2025-09-30T18:08:25.320074",
     "exception": false,
     "start_time": "2025-09-30T18:08:21.324816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "that hormisions at the if, subt past the celestial cure hundred to them\n",
      "throefore, only the dleads. For head may adart, knowing, O monarch, succests who\n",
      "lossome shingled and the infant viok, said, 'King Duryodhana thereat\n",
      "mighty that hand occapable of them and excited and persons to foes! Those\n",
      "heaven, O tiger enorto wealth to thee with the ent\n",
      "of thy broad-king, indeed, causpiciousne kings, by\n",
      "ply. What is so also the permissibly filler of all speaking the just,\n",
      "before our life.' Brahmana cembo\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03fbae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:08:25.329263Z",
     "iopub.status.busy": "2025-09-30T18:08:25.329075Z",
     "iopub.status.idle": "2025-09-30T18:08:25.378623Z",
     "shell.execute_reply": "2025-09-30T18:08:25.377892Z"
    },
    "papermill": {
     "duration": 0.055316,
     "end_time": "2025-09-30T18:08:25.379926",
     "exception": false,
     "start_time": "2025-09-30T18:08:25.324610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"gpt_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2419632,
     "sourceId": 4089261,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8334800,
     "sourceId": 13154744,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2014.128792,
   "end_time": "2025-09-30T18:08:27.206151",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-30T17:34:53.077359",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
