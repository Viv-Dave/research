{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toy implementation of BERT from scratch to build understanding of encoder transformer architecture and it's comparison to GPT-1.\n",
    "### Papers referenced: BERT, Devlin et. al & Attention is all you need Vaswani et. al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.147758Z",
     "iopub.status.busy": "2025-10-05T13:43:05.147489Z",
     "iopub.status.idle": "2025-10-05T13:43:05.151886Z",
     "shell.execute_reply": "2025-10-05T13:43:05.151371Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.147739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast, BertForPreTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Manipulation for Model Training**\n",
    "\n",
    "Here we create the textual and positional embeddings of the data we need to train our BERT on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.157499Z",
     "iopub.status.busy": "2025-10-05T13:43:05.156830Z",
     "iopub.status.idle": "2025-10-05T13:43:05.169774Z",
     "shell.execute_reply": "2025-10-05T13:43:05.169228Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.157482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Use cuda if available or else stay on CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.170941Z",
     "iopub.status.busy": "2025-10-05T13:43:05.170706Z",
     "iopub.status.idle": "2025-10-05T13:43:05.475149Z",
     "shell.execute_reply": "2025-10-05T13:43:05.474550Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.170917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#A pre-trained tokenizer to tokenize the input given to the model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.477151Z",
     "iopub.status.busy": "2025-10-05T13:43:05.476445Z",
     "iopub.status.idle": "2025-10-05T13:43:05.493988Z",
     "shell.execute_reply": "2025-10-05T13:43:05.493213Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.477131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Total Model Parameters: 33.67M\n",
    "\"\"\"\n",
    "n_layers = 6\n",
    "n_heads = 6\n",
    "block_size = 32\n",
    "batch_size = 8\n",
    "n_embd = 384\n",
    "dropout = 0.1\n",
    "max_iters = 4500\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.495020Z",
     "iopub.status.busy": "2025-10-05T13:43:05.494776Z",
     "iopub.status.idle": "2025-10-05T13:43:05.707343Z",
     "shell.execute_reply": "2025-10-05T13:43:05.706689Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.494997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Dataset preparation\n",
    "PATH = \"/kaggle/input/shakespeare6/shakespeare.txt\"\n",
    "WIKI_PATH = \"/kaggle/input/wikitext2-data/train.txt\"\n",
    "with open(WIKI_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.709044Z",
     "iopub.status.busy": "2025-10-05T13:43:05.708839Z",
     "iopub.status.idle": "2025-10-05T13:43:05.719597Z",
     "shell.execute_reply": "2025-10-05T13:43:05.719017Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.709028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def create_pretraining_batch(sentences, tokenizer, batch_size=32, max_length=64, mask_prob=0.15, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Create a single batch for BERT pretraining using BertTokenizerFast + BertForPreTraining.\n",
    "    \n",
    "    Returns a dict with:\n",
    "        - input_ids\n",
    "        - attention_mask\n",
    "        - token_type_ids\n",
    "        - labels_mlm\n",
    "        - labels_nsp\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_a_batch = []\n",
    "    sentence_b_batch = []\n",
    "    nsp_labels = []\n",
    "    i = 0\n",
    "    while len(sentence_a_batch) < batch_size and i < len(sentences) - 1:\n",
    "        a = sentences[i]\n",
    "\n",
    "        if random.random() < 0.5 and i < len(sentences) - 1:\n",
    "            # IsNext\n",
    "            b = sentences[i + 1]\n",
    "            label = 0\n",
    "        else:\n",
    "            # NotNext\n",
    "            rand_idx = random.randint(0, len(sentences) - 1)\n",
    "            while rand_idx in [i, i + 1]:\n",
    "                rand_idx = random.randint(0, len(sentences) - 1)\n",
    "            b = sentences[rand_idx]\n",
    "            label = 1\n",
    "\n",
    "        sentence_a_batch.append(a)\n",
    "        sentence_b_batch.append(b)\n",
    "        nsp_labels.append(label)\n",
    "        i += 1\n",
    "\n",
    "    labels_nsp = torch.tensor(nsp_labels, dtype=torch.long, device=device)\n",
    "\n",
    "    # Step 2: Tokenize batch\n",
    "    encoding = tokenizer(\n",
    "        list(zip(sentence_a_batch, sentence_b_batch)),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    token_type_ids = encoding[\"token_type_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Step 3: Create MLM labels\n",
    "    labels_mlm = input_ids.clone()\n",
    "    masked_input = input_ids.clone()\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Initialize labels: -100 for all positions first\n",
    "    labels_mlm[:] = -100\n",
    "\n",
    "    for i in range(masked_input.size(0)):\n",
    "        tokens = masked_input[i]\n",
    "\n",
    "        # Candidate positions for masking (exclude special tokens)\n",
    "        candidate_mask = (tokens != cls_id) & (tokens != sep_id) & (tokens != pad_id)\n",
    "        candidate_indices = candidate_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(candidate_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        # Number of tokens to mask\n",
    "        num_to_mask = max(1, int(mask_prob * len(candidate_indices)))\n",
    "        mask_indices = candidate_indices[torch.randperm(len(candidate_indices))[:num_to_mask]]\n",
    "\n",
    "        # Fill MLM labels at masked positions\n",
    "        labels_mlm[i, mask_indices] = input_ids[i, mask_indices]\n",
    "\n",
    "        # Apply masking 80% [MASK], 10% random token, 10% original\n",
    "        for idx in mask_indices:\n",
    "            prob = random.random()\n",
    "            if prob < 0.8:\n",
    "                tokens[idx] = mask_token_id\n",
    "            elif prob < 0.9:\n",
    "                tokens[idx] = random.randint(0, vocab_size - 1)\n",
    "            # else: leave original\n",
    "\n",
    "        masked_input[i] = tokens\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": masked_input,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels_mlm\": labels_mlm,\n",
    "        \"labels_nsp\": labels_nsp\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Attention Mechanism for the BERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.720601Z",
     "iopub.status.busy": "2025-10-05T13:43:05.720354Z",
     "iopub.status.idle": "2025-10-05T13:43:05.743629Z",
     "shell.execute_reply": "2025-10-05T13:43:05.743068Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.720579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "        \"\"\"\n",
    "        The Self-Attention Head:\n",
    "        Pass query, key and value through the weights initialized.\n",
    "        self-attention = softmax([Q @ K^T/root(d_k)]) @ V\n",
    "        Multiply Query and Key values, normalize them by the size of key, apply softmax\n",
    "        to get attention weights and then perform matmul\n",
    "        between the obtained attn_weights and the values.\n",
    "        The output is a weighted sum of values.\n",
    "        \"\"\"\n",
    "        def __init__(self, head_size):\n",
    "            super(AttentionHead, self).__init__()\n",
    "            self.w_q = nn.Linear(n_embd, head_size)\n",
    "            self.w_k = nn.Linear(n_embd, head_size)\n",
    "            self.w_v = nn.Linear(n_embd, head_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        def forward(self,x):\n",
    "            q = self.w_q(x) \n",
    "            k = self.w_k(x) \n",
    "            v = self.w_v(x) \n",
    "\n",
    "            scores = q @ k.transpose(-2,-1)\n",
    "            scores = scores * (k.size(-1)**-0.5)\n",
    "            attention_weights = F.softmax(scores, dim=-1) \n",
    "            attention_weights = self.dropout(attention_weights)\n",
    "            output = attention_weights @ v\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.744623Z",
     "iopub.status.busy": "2025-10-05T13:43:05.744394Z",
     "iopub.status.idle": "2025-10-05T13:43:05.766738Z",
     "shell.execute_reply": "2025-10-05T13:43:05.766209Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.744598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Self-Attention using multiple heads over the input x.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.767625Z",
     "iopub.status.busy": "2025-10-05T13:43:05.767373Z",
     "iopub.status.idle": "2025-10-05T13:43:05.785908Z",
     "shell.execute_reply": "2025-10-05T13:43:05.785382Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.767608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    The FeedForward Network takes the input x, it scales it upto 4 times the embedding dimensions.\n",
    "    Applies reLU activation\n",
    "    Downscale the activated input back to its original embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.FF = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out = self.FF(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.786951Z",
     "iopub.status.busy": "2025-10-05T13:43:05.786705Z",
     "iopub.status.idle": "2025-10-05T13:43:05.803521Z",
     "shell.execute_reply": "2025-10-05T13:43:05.802981Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.786928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd) \n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(x) \n",
    "        x = self.ln1(x)    \n",
    "\n",
    "        x = x + self.ffwd(x) \n",
    "        x = self.ln2(x)    \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.804476Z",
     "iopub.status.busy": "2025-10-05T13:43:05.804251Z",
     "iopub.status.idle": "2025-10-05T13:43:05.835743Z",
     "shell.execute_reply": "2025-10-05T13:43:05.835220Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.804442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super(BERT, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embeddings = nn.Embedding(block_size, n_embd)\n",
    "        self.seg_embeddings = nn.Embedding(2, n_embd)\n",
    "        self.enc_layers = nn.Sequential(\n",
    "            EncoderBlock(n_embd, n_head),\n",
    "            EncoderBlock(n_embd, n_head),\n",
    "            EncoderBlock(n_embd, n_head),\n",
    "            EncoderBlock(n_embd, n_head),\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "        self.mlm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.nsp_head = nn.Linear(n_embd, 2)\n",
    "    def forward(self,idx, segment_ids, targets=None, nsp_labels=None):\n",
    "        B,T=idx.shape\n",
    "        token_emb = self.token_embeddings(idx)\n",
    "        pos_emb = self.pos_embeddings(torch.arange(T, device=device))\n",
    "        seg_emb = self.seg_embeddings(segment_ids)\n",
    "        x = token_emb+pos_emb+seg_emb\n",
    "        x = self.enc_layers(x)\n",
    "        mlm_logits = self.mlm_head(x)\n",
    "        cls_token_hidden = x[:, 0] \n",
    "        nsp_logits = self.nsp_head(cls_token_hidden)\n",
    "        total_loss, mlm_loss, nsp_loss = None, None, None\n",
    "        if targets is not None:\n",
    "            mlm_loss = F.cross_entropy(\n",
    "                mlm_logits.view(-1, mlm_logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-100 \n",
    "            )\n",
    "        if nsp_labels is not None:\n",
    "            nsp_loss = F.cross_entropy(nsp_logits, nsp_labels)\n",
    "\n",
    "        if mlm_loss is not None and nsp_loss is not None:\n",
    "            total_loss = mlm_loss + nsp_loss\n",
    "\n",
    "        return mlm_logits, nsp_logits, total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:05.837942Z",
     "iopub.status.busy": "2025-10-05T13:43:05.837773Z",
     "iopub.status.idle": "2025-10-05T13:43:06.178333Z",
     "shell.execute_reply": "2025-10-05T13:43:06.177708Z",
     "shell.execute_reply.started": "2025-10-05T13:43:05.837929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = BERT(vocab_size=vocab_size, n_embd=n_embd, block_size=MAX_SEQUENCE_LENGTH,n_head=n_heads, n_layer=n_layers)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:06.179326Z",
     "iopub.status.busy": "2025-10-05T13:43:06.179023Z",
     "iopub.status.idle": "2025-10-05T13:43:06.185799Z",
     "shell.execute_reply": "2025-10-05T13:43:06.185095Z",
     "shell.execute_reply.started": "2025-10-05T13:43:06.179300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:43:06.187199Z",
     "iopub.status.busy": "2025-10-05T13:43:06.186943Z",
     "iopub.status.idle": "2025-10-05T13:55:49.846371Z",
     "shell.execute_reply": "2025-10-05T13:55:49.845736Z",
     "shell.execute_reply.started": "2025-10-05T13:43:06.187156Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0000 | Loss: 11.1309\n",
      "Step 0200 | Loss: 5.8234\n",
      "Step 0400 | Loss: 4.6646\n",
      "Step 0600 | Loss: 3.8559\n",
      "Step 0800 | Loss: 3.0014\n",
      "Step 1000 | Loss: 2.2054\n",
      "Step 1200 | Loss: 1.4703\n",
      "Step 1400 | Loss: 2.0974\n",
      "Step 1600 | Loss: 2.5161\n",
      "Step 1800 | Loss: 2.5515\n",
      "Step 2000 | Loss: 2.0625\n",
      "Step 2200 | Loss: 2.9385\n",
      "Step 2400 | Loss: 1.3653\n",
      "Step 2600 | Loss: 2.0359\n",
      "Step 2800 | Loss: 1.5512\n",
      "Step 3000 | Loss: 2.1846\n",
      "Step 3200 | Loss: 2.4339\n",
      "Step 3400 | Loss: 1.9960\n",
      "Step 3600 | Loss: 1.2103\n",
      "Step 3800 | Loss: 1.6484\n",
      "Step 4000 | Loss: 1.9599\n",
      "Step 4200 | Loss: 1.4479\n",
      "Step 4400 | Loss: 1.7401\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for step in range(max_iters):\n",
    "    batch = create_pretraining_batch(text.splitlines(), tokenizer, batch_size=32, max_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels_mlm = batch[\"labels_mlm\"].to(device)\n",
    "    labels_nsp = batch[\"labels_nsp\"].to(device)\n",
    "\n",
    "    mlm_logits, nsp_logits, loss = model(\n",
    "        idx=input_ids,\n",
    "        segment_ids=token_type_ids,\n",
    "        targets=labels_mlm,\n",
    "        nsp_labels=labels_nsp\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step:04d} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:55:49.847380Z",
     "iopub.status.busy": "2025-10-05T13:55:49.847108Z",
     "iopub.status.idle": "2025-10-05T13:55:49.852705Z",
     "shell.execute_reply": "2025-10-05T13:55:49.852098Z",
     "shell.execute_reply.started": "2025-10-05T13:55:49.847361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_layers = 4\\nn_heads = 4\\nblock_size = 8\\nbatch_size = 8\\nn_embd = 384\\ndropout = 0.1\\nmax_iters = 4500\\nMAX_SEQUENCE_LENGTH = 64\\nfINAL LOSS: Step 4400 | Loss: 1.4863\\n## Trained on : Shakespeare\\nlr = 5e-4\\n\\nPART 2:\\nn_layers = 6\\nn_heads = 6\\nblock_size = 32\\nbatch_size = 8\\nn_embd = 384\\ndropout = 0.1\\nmax_iters = 4500\\nMAX_SEQUENCE_LENGTH = 128\\nvocab_size = len(tokenizer.get_vocab())\\nprint(vocab_size)\\nStep 4400 | Loss: 2.2908\\nLR = 1e-4\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "block_size = 8\n",
    "batch_size = 8\n",
    "n_embd = 384\n",
    "dropout = 0.1\n",
    "max_iters = 4500\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "fINAL LOSS: Step 4400 | Loss: 1.4863\n",
    "## Trained on : Shakespeare\n",
    "lr = 5e-4\n",
    "\n",
    "PART 2:\n",
    "n_layers = 6\n",
    "n_heads = 6\n",
    "block_size = 32\n",
    "batch_size = 8\n",
    "n_embd = 384\n",
    "dropout = 0.1\n",
    "max_iters = 4500\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(vocab_size)\n",
    "Step 4400 | Loss: 2.2908\n",
    "LR = 1e-4\n",
    "## Trained on : Shakespeare\n",
    "\n",
    "PART 3:\n",
    "n_layers = 6\n",
    "n_heads = 6\n",
    "block_size = 32\n",
    "batch_size = 8\n",
    "n_embd = 384\n",
    "dropout = 0.1\n",
    "max_iters = 4500\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(vocab_size)\n",
    "Step 4400 | Loss: 2.2908\n",
    "LR = 1e-4\n",
    "## Trained on: wiki-text2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:56:33.418849Z",
     "iopub.status.busy": "2025-10-05T13:56:33.418240Z",
     "iopub.status.idle": "2025-10-05T13:56:33.770081Z",
     "shell.execute_reply": "2025-10-05T13:56:33.769499Z",
     "shell.execute_reply.started": "2025-10-05T13:56:33.418827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bert__wiki_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 461948,
     "sourceId": 868991,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1480173,
     "sourceId": 2446022,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2419632,
     "sourceId": 4089261,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
